---
title: "Análisis de Modelos"
author: "Gustavo Cruz; Pedro Guzman"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Carga de Librerías y Datos

```{r}
library(tidyverse)
library(caret)
library(car)
library(Metrics)
library(ggplot2)

train <- read.csv("train.csv")
test <- read.csv("test.csv")

set.seed(2301)
trainIndex <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)
train_set <- train[trainIndex, ]
test_set <- train[-trainIndex, ]
```

## Construcción del Modelo

```{r}
lm_opt <- lm(SalePrice ~ GrLivArea + OverallQual + YearBuilt + GarageCars, data = train_set)
summary(lm_opt)
```

## Análisis de los Residuos

```{r}
par(mfrow = c(2, 2))
plot(lm_opt)
par(mfrow = c(1, 1))
```

## **Interpretación de las graficas:**
**En la primera gráfica** (Residuals vs Fitted), observamos que los residuos no están completamente dispersos de manera aleatoria alrededor de la línea horizontal en cero. Hay una ligera curvatura, lo que sugiere una posible no linealidad en los datos. Esto indica que el modelo puede no capturar completamente la relación entre las variables predictoras y el precio de venta.

**La segunda gráfica** (Q-Q Plot) muestra la distribución de los residuos en comparación con una distribución normal teórica. Vemos que los puntos se desvían en los extremos, lo que indica la presencia de colas gruesas y sugiere que los residuos no siguen perfectamente una distribución normal. Esto puede afectar la validez de los intervalos de confianza y pruebas estadísticas en el modelo.

**En la tercera gráfica** (Scale-Location), los residuos tienden a aumentar en variabilidad a medida que aumentan los valores ajustados. Esto sugiere heterocedasticidad, es decir, la varianza de los errores no es constante. Idealmente, deberíamos ver una distribución uniforme en la dispersión de los puntos.

**La gráfica de Residuals vs Leverage** nos ayuda a identificar puntos influyentes en el modelo. Se observan algunos puntos con alta influencia, señalados por el umbral de Cook's Distance. Estos valores atípicos pueden estar afectando los coeficientes del modelo, y se recomienda analizarlos individualmente para decidir si deben ser eliminados o si es necesario un modelo más robusto.

## Evaluación del Desempeño del Modelo

```{r}
preds <- predict(lm_opt, newdata = test_set)

mae_val <- mae(test_set$SalePrice, preds)
rmse_val <- rmse(test_set$SalePrice, preds)
r2_val <- cor(test_set$SalePrice, preds)^2

cat("MAE:", mae_val, "\n")
cat("RMSE:", rmse_val, "\n")
cat("R²:", r2_val, "\n")
```

Para evaluar la precisión del modelo, utilizamos tres métricas: MAE (Error Absoluto Medio), RMSE (Raíz del Error Cuadrático Medio) y R² (Coeficiente de Determinación).

- MAE: 25,446.3 → En promedio, el modelo tiene un error absoluto de aproximadamente 25,446 dólares en la predicción del precio de venta. Esto indica el margen de error esperado en una estimación individual.
- RMSE: 38,687.16 → El RMSE mide la dispersión de los errores y penaliza más los errores grandes. Un RMSE más bajo es mejor, pero en este caso indica que aún hay errores significativos en la predicción de los precios.
- R²: 0.7155 → Esto significa que el modelo explica aproximadamente el 71.55% de la variabilidad en los precios de venta de las propiedades. Aunque es un buen valor, todavía hay un 28.45% de variabilidad no explicada, lo que sugiere que otras variables podrían mejorar el modelo.

## División en Conjuntos de Entrenamiento y Prueba para Análisis Adicional
```{r}
set.seed(3915)
trainIndex <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)
train_set <- train[trainIndex, ]
test_set <- train[-trainIndex, ]
```

## Eliminación de variables con un solo nivel
```{r}
unique_counts <- sapply(train_set, function(x) length(unique(x)))
problematic_vars <- names(unique_counts[unique_counts == 1])
if(length(problematic_vars) > 0) {
  train_set <- train_set[, !(names(train_set) %in% problematic_vars)]
  test_set <- test_set[, !(names(test_set) %in% problematic_vars)]
  cat("Variables eliminadas por tener un solo nivel:", problematic_vars, "\n")
} else {
  cat("No se encontraron variables con un solo nivel.\n")
}
```

## Eliminación de variables categóricas con un solo nivel
```{r}
# Identificar variables de tipo factor
cat_vars <- names(which(sapply(train_set, is.factor)))

# Verificar si hay factores con un solo nivel
if(length(cat_vars) > 0) {
  cat_vars_unicas <- character(0)
  for(col in cat_vars) {
    if(length(unique(train_set[[col]])) == 1) {
      cat_vars_unicas <- c(cat_vars_unicas, col)
    }
  }
  
  if(length(cat_vars_unicas) > 0) {
    train_set <- train_set[, !(names(train_set) %in% cat_vars_unicas)]
    test_set <- test_set[, !(names(test_set) %in% cat_vars_unicas)]
    cat("Variables categóricas eliminadas por tener un solo nivel:", cat_vars_unicas, "\n")
  } else {
    cat("No se encontraron variables categóricas con un solo nivel.\n")
  }
} else {
  cat("No se encontraron variables de tipo factor en el conjunto de datos.\n")
}
```

## Manejo de Valores Faltantes
```{r}
# Verificamos valores faltantes en conjuntos de entrenamiento y prueba
na_count_train <- colSums(is.na(train_set))
na_count_test <- colSums(is.na(test_set))

# Mostramos las variables con valores faltantes
na_vars_train <- names(na_count_train[na_count_train > 0])
na_vars_test <- names(na_count_test[na_count_test > 0])

cat("Variables con valores faltantes en train_set:", 
    ifelse(length(na_vars_train) > 0, paste(na_vars_train, collapse=", "), "Ninguna"), "\n")
cat("Variables con valores faltantes en test_set:", 
    ifelse(length(na_vars_test) > 0, paste(na_vars_test, collapse=", "), "Ninguna"), "\n")

# Imputación simple para variables numéricas (media)
for(col in na_vars_train) {
  if(is.numeric(train_set[[col]])) {
    mean_val <- mean(train_set[[col]], na.rm = TRUE)
    train_set[[col]][is.na(train_set[[col]])] <- mean_val
    if(col %in% names(test_set)) {
      test_set[[col]][is.na(test_set[[col]])] <- mean_val
    }
  } else if(is.factor(train_set[[col]])) {
    # Imputación para variables categóricas (moda)
    levels_count <- table(train_set[[col]], useNA = "no")
    if(length(levels_count) > 0) {
      mode_val <- names(which.max(levels_count))
      train_set[[col]][is.na(train_set[[col]])] <- mode_val
      if(col %in% names(test_set)) {
        test_set[[col]][is.na(test_set[[col]])] <- mode_val
      }
    }
  }
}
```

## Definición de Modelos de Regresión
```{r}
# Aseguramos que las variables predictoras son equivalentes en ambos conjuntos
common_vars <- intersect(names(train_set), names(test_set))
train_set <- train_set[, common_vars]
test_set <- test_set[, common_vars]

# Modelo 1: Regresión Simple
lm1 <- lm(SalePrice ~ GrLivArea, data = train_set)
summary(lm1)

# Modelo 2: Regresión Múltiple con variables seleccionadas
lm2 <- lm(SalePrice ~ GrLivArea + OverallQual + YearBuilt + GarageCars, data = train_set)
summary(lm2)

```

## Evaluación de Modelos
```{r}
modelos <- list(lm1, lm2)
nombres_modelos <- c("Regresión Simple", "Regresión Múltiple", "Modelo Completo")
resultados <- data.frame(Modelo = character(), MAE = numeric(), RMSE = numeric(), R2 = numeric(), stringsAsFactors = FALSE)

for (i in 1:length(modelos)) {
  tryCatch({
    preds <- predict(modelos[[i]], newdata = test_set)
    mae_val <- mae(test_set$SalePrice, preds)
    rmse_val <- rmse(test_set$SalePrice, preds)
    r2_val <- cor(test_set$SalePrice, preds)^2
    
    resultados <- rbind(resultados, data.frame(
      Modelo = nombres_modelos[i],
      MAE = mae_val,
      RMSE = rmse_val,
      R2 = r2_val,
      stringsAsFactors = FALSE
    ))
  }, error = function(e) {
    cat("Error al evaluar el modelo", nombres_modelos[i], ":", e$message, "\n")
  })
}
```

## Visualización de Resultados
```{r}
if(nrow(resultados) > 0) {
  print(resultados)
  
  # Gráfico de barras para RMSE
  ggplot(resultados, aes(x = Modelo, y = RMSE, fill = Modelo)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(title = "Comparación de Modelos - RMSE", y = "RMSE", x = "Modelo") +
    theme(legend.position = "none")
  
  # Gráfico de barras para R²
  ggplot(resultados, aes(x = Modelo, y = R2, fill = Modelo)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(title = "Comparación de Modelos - R²", y = "R²", x = "Modelo") +
    theme(legend.position = "none")
} else {
  cat("No se pudieron evaluar los modelos correctamente.\n")
}
```

## Diagnóstico de Residuos para el Modelo Completo
```{r}
tryCatch({
  par(mfrow=c(2,2))
  plot(lm_full)
  par(mfrow=c(1,1))
}, error = function(e) {
  cat("Error al generar los gráficos de diagnóstico:", e$message, "\n")
})
```

## Análisis de Importancia de Variables (para el Modelo Completo)
```{r}
tryCatch({
  # Obtenemos los coeficientes del modelo completo
  coefs <- coef(lm_full)[-1]  # Excluimos el intercepto
  
  # Aseguramos que tenemos coeficientes para procesar
  if(length(coefs) > 0) {
    var_importance <- abs(coefs)
    var_importance <- sort(var_importance, decreasing = TRUE)
    
    # Visualizamos las 15 variables más importantes o todas si hay menos
    n_vars <- min(15, length(var_importance))
    head_vars <- head(var_importance, n_vars)
    importance_df <- data.frame(
      Variable = names(head_vars),
      Importancia = as.numeric(head_vars),
      stringsAsFactors = FALSE
    )
    
    ggplot(importance_df, aes(x = reorder(Variable, Importancia), y = Importancia)) +
      geom_bar(stat = "identity", fill = "steelblue") +
      coord_flip() +
      theme_minimal() +
      labs(title = "Variables más Importantes en el Modelo", 
           x = "Variable", 
           y = "Importancia (|Coeficiente|)")
  } else {
    cat("No se encontraron coeficientes para analizar la importancia de variables.\n")
  }
}, error = function(e) {
  cat("Error al analizar la importancia de variables:", e$message, "\n")
})
```

## Conclusiones
```{r, results='asis'}
cat("## Conclusiones\n\n")

if(nrow(resultados) > 0) {
  tryCatch({
    cat("- El modelo completo obtuvo un RMSE de", round(resultados$RMSE[3], 2), 
        "y un R² de", round(resultados$R2[3], 3), "en el conjunto de prueba.\n")
    
    # Verificamos si podemos acceder a var_importance
    if(exists("var_importance") && length(var_importance) >= 5) {
      cat("- Las variables más influyentes en el precio de venta incluyen: ", 
          paste(names(head(var_importance, 5)), collapse=", "), ".\n")
    }
    
    mejor_modelo_idx <- which.min(resultados$RMSE)
    cat("- El modelo con mejor desempeño fue '", resultados$Modelo[mejor_modelo_idx], 
        "' con un RMSE de ", round(resultados$RMSE[mejor_modelo_idx], 2), 
        " y un R² de ", round(resultados$R2[mejor_modelo_idx], 3), ".\n")
    
    # Test de normalidad solo si tenemos el modelo completo
    if(exists("lm_full")) {
      tryCatch({
        normal_test <- shapiro.test(residuals(lm_full))
        cat("- Los diagnósticos de residuos sugieren que ", 
            ifelse(normal_test$p.value > 0.05, 
                   "los residuos siguen una distribución aproximadamente normal.", 
                   "podría haber algunas desviaciones de la normalidad en los residuos."), "\n")
      }, error = function(e) {
        cat("- No se pudo realizar la prueba de normalidad en los residuos.\n")
      })
    }
  }, error = function(e) {
    cat("Error al generar las conclusiones:", e$message, "\n")
  })
} else {
  cat("No se pudieron generar conclusiones debido a errores en la evaluación de los modelos.\n")
}
```
